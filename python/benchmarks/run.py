#!/usr/bin/env python3
"""
Offline tokenizer benchmark — no server needed.

Compares gpt2, bert-base-uncased, and fixed 30-char chunking across:
  - token count, avg chars/token, tokenize time, tokens/sec
  - simulated streaming throughput (file writes, no delay)

Usage: uv run --project python python/benchmarks/run.py
"""
import sys, time, tempfile, os
from tokenizers import Tokenizer

sys.path.insert(0, __file__.rsplit("/", 2)[0])
from massive import build_html, CHUNK_SIZE

TOKENIZERS = ["gpt2", "bert-base-uncased"]
N_REPS     = 100


def bench_tokenizer(name: str, html: str) -> dict:
    print(f"  Loading {name}...", end=" ", flush=True)
    tok = Tokenizer.from_pretrained(name)
    print("done")

    # Warm-up
    tok.encode(html)

    # Time N_REPS encodes
    t0 = time.perf_counter()
    for _ in range(N_REPS):
        enc = tok.encode(html)
    elapsed = time.perf_counter() - t0

    ids    = enc.ids
    n_tok  = len(ids)
    avg_ms = (elapsed / N_REPS) * 1000
    tps    = n_tok / (elapsed / N_REPS)
    avg_ch = len(html) / n_tok if n_tok else 0

    # Simulate streaming (file writes, no delay)
    tokens = [tok.decode([id]) for id in ids]
    with tempfile.NamedTemporaryFile(mode="w", suffix=".html", delete=False) as f:
        tmp = f.name
        t1 = time.perf_counter()
        for token in tokens:
            f.write(token)
            f.flush()
        stream_elapsed = time.perf_counter() - t1
    os.unlink(tmp)

    kb   = len(html.encode()) / 1024
    kbps = kb / stream_elapsed if stream_elapsed > 0 else 0
    sse  = max(1, int(stream_elapsed / 0.1))

    return {
        "name":           name,
        "n_tok":          n_tok,
        "avg_ch":         avg_ch,
        "tok_ms":         avg_ms,
        "tps":            tps,
        "stream_elapsed": stream_elapsed,
        "kbps":           kbps,
        "flushes":        n_tok,
        "sse":            sse,
    }


def bench_fixed(html: str) -> dict:
    chunk = CHUNK_SIZE
    n_chunks = len(range(0, len(html), chunk))

    # Time N_REPS chunking passes
    t0 = time.perf_counter()
    for _ in range(N_REPS):
        chunks = [html[i:i+chunk] for i in range(0, len(html), chunk)]
    elapsed = time.perf_counter() - t0

    avg_ms = (elapsed / N_REPS) * 1000
    tps    = n_chunks / (elapsed / N_REPS)

    # Simulate streaming
    with tempfile.NamedTemporaryFile(mode="w", suffix=".html", delete=False) as f:
        tmp = f.name
        t1 = time.perf_counter()
        for ch in chunks:
            f.write(ch)
            f.flush()
        stream_elapsed = time.perf_counter() - t1
    os.unlink(tmp)

    kb   = len(html.encode()) / 1024
    kbps = kb / stream_elapsed if stream_elapsed > 0 else 0
    sse  = max(1, int(stream_elapsed / 0.1))

    return {
        "name":           f"Fixed {chunk}-char chunks",
        "n_tok":          n_chunks,
        "avg_ch":         chunk,
        "tok_ms":         avg_ms,
        "tps":            tps,
        "stream_elapsed": stream_elapsed,
        "kbps":           kbps,
        "flushes":        n_chunks,
        "sse":            sse,
    }


def fmt_k(n: float) -> str:
    return f"{n/1000:.0f}k" if n >= 1000 else str(int(n))


def main():
    print("Building HTML corpus...", end=" ", flush=True)
    html = build_html()
    print(f"done  ({len(html):,} chars / {len(html.encode()):,} bytes)")
    print()

    print(f"Benchmarking (tokenize × {N_REPS} reps each):")
    results = []
    for name in TOKENIZERS:
        results.append(bench_tokenizer(name, html))
    results.append(bench_fixed(html))

    # ── tokenization table ─────────────────────────────────────────────────────
    print()
    print("─" * 82)
    print(f"{'Tokenizer':<26} {'Tokens':>8} {'Avg ch/tok':>11} {'Tok ms':>9} {'Tokens/sec':>12}")
    print("─" * 82)
    for r in results:
        print(
            f"{r['name']:<26} {r['n_tok']:>8,} {r['avg_ch']:>11.1f}"
            f" {r['tok_ms']:>9.1f} {fmt_k(r['tps']):>12}"
        )
    print("─" * 82)

    # ── streaming simulation table ─────────────────────────────────────────────
    print()
    print("─" * 82)
    print(f"{'Tokenizer':<26} {'Flushes':>8} {'Elapsed s':>10} {'KB/s':>8} {'SSE est':>9}")
    print("─" * 82)
    for r in results:
        print(
            f"{r['name']:<26} {r['flushes']:>8,} {r['stream_elapsed']:>10.3f}"
            f" {r['kbps']:>8.1f} {r['sse']:>9,}"
        )
    print("─" * 82)


if __name__ == "__main__":
    main()
